"""
æ–‡ç« æ‘˜è¦ç”Ÿæˆå™¨
è¯»å–æ˜¨å¤©çš„æ–‡ç« ï¼Œä½¿ç”¨ DeepSeek LLM ç”Ÿæˆæ‘˜è¦
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
ARTICLES_DIR = Path("articles")
SUMMARY_DIR = Path("summary")
SUMMARY_DIR.mkdir(exist_ok=True)

PROMPT_FILE = Path("prompt.txt")

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"

# API è°ƒç”¨é…ç½®
API_TIMEOUT = 120  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 3    # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 5    # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰


def get_yesterday_date() -> str:
    """è·å–æ˜¨å¤©çš„æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD"""
    yesterday = datetime.now() - timedelta(days=1)
    return yesterday.strftime("%Y%m%d")


def load_prompt() -> str:
    """åŠ è½½æç¤ºè¯"""
    if PROMPT_FILE.exists():
        with open(PROMPT_FILE, "r", encoding="utf-8") as f:
            return f.read().strip()
    else:
        # é»˜è®¤æç¤ºè¯
        return "è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼Œä¸è¶…è¿‡200å­—ï¼š"


def load_articles(date_str: str) -> list[dict]:
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ–‡ç« """
    articles_path = ARTICLES_DIR / date_str
    
    if not articles_path.exists():
        print(f"ç›®å½•ä¸å­˜åœ¨: {articles_path}")
        return []
    
    articles = []
    for file_path in articles_path.glob("*.json"):
        # è·³è¿‡ article_links.json ç­‰éæ–‡ç« æ–‡ä»¶
        if file_path.name.startswith("article_links") or file_path.name.startswith("all_articles"):
            continue
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                article = json.load(f)
                article["_file_path"] = str(file_path)
                articles.append(article)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    return articles


class SummaryGenerationError(Exception):
    """æ‘˜è¦ç”Ÿæˆå¤±è´¥å¼‚å¸¸"""
    pass


def estimate_tokens(text: str) -> int:
    """
    ç²—ç•¥ä¼°ç®—æ–‡æœ¬çš„ token æ•°
    ä¸­æ–‡çº¦ 1.5 å­—ç¬¦ = 1 tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦ = 1 token
    """
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    other_chars = len(text) - chinese_chars
    return int(chinese_chars / 1.5 + other_chars / 4)


def generate_summary(client: OpenAI, prompt: str, article: dict) -> tuple[str, int]:
    """
    è°ƒç”¨ DeepSeek API ç”Ÿæˆæ‘˜è¦ï¼Œå¸¦é‡è¯•æœºåˆ¶
    
    Returns:
        tuple: (æ‘˜è¦æ–‡æœ¬, é¢„ä¼°tokenæ•°)
    
    Raises:
        SummaryGenerationError: é‡è¯•å¤šæ¬¡åä»ç„¶å¤±è´¥
    """
    title = article.get("title", "æ— æ ‡é¢˜")
    content = article.get("content", "")
    
    if not content:
        return "ï¼ˆæ–‡ç« å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ï¼‰", 0
    
    # æ„å»ºæ¶ˆæ¯
    user_message = f"{prompt}\n\næ ‡é¢˜ï¼š{title}\n\næ­£æ–‡ï¼š\n{content}"
    
    last_error = None
    
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": user_message}
                ],
                max_tokens=4096,
                temperature=0.2,
                timeout=API_TIMEOUT,
            )
            
            summary = response.choices[0].message.content.strip()
            
            # å°è¯•ä»å“åº”è·å–å®é™… token æ•°ï¼Œå¦åˆ™ä¼°ç®—
            if hasattr(response, 'usage') and response.usage:
                total_tokens = response.usage.total_tokens
            else:
                # ç²—ç•¥ä¼°ç®—ï¼šè¾“å…¥ + è¾“å‡º
                total_tokens = estimate_tokens(user_message) + estimate_tokens(summary)
            
            return summary, total_tokens
        
        except Exception as e:
            last_error = e
            error_msg = str(e).lower()
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯è¶…æ—¶æˆ–ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œè¿›è¡Œé‡è¯•
            if 'timeout' in error_msg or 'timed out' in error_msg or 'connection' in error_msg:
                print(f"  âš  API è¯·æ±‚è¶…æ—¶ï¼Œç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡é‡è¯•...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise SummaryGenerationError(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
    
    # é‡è¯•æ¬¡æ•°ç”¨å®Œä»ç„¶å¤±è´¥
    raise SummaryGenerationError(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥ï¼ˆé‡è¯• {MAX_RETRIES} æ¬¡åï¼‰: {last_error}")


def format_summary_output(articles_with_summary: list[dict], date_str: str) -> str:
    """æ ¼å¼åŒ–æ‘˜è¦è¾“å‡º"""
    lines = []
    separator = "\n" + "=" * 80 + "\n"
    
    # è§£ææ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD -> xxxxå¹´xxæœˆxxæ—¥)
    year = date_str[:4]
    month = date_str[4:6].lstrip('0')  # å»æ‰å‰å¯¼é›¶
    day = date_str[6:8].lstrip('0')    # å»æ‰å‰å¯¼é›¶
    formatted_date = f"{year}å¹´{month}æœˆ{day}æ—¥"
    
    for i, item in enumerate(articles_with_summary, 1):
        title = item.get("title", "æ— æ ‡é¢˜")
        summary = item.get("summary", "")
        url = item.get("url", "")
        
        article_block = f"""ã€{i}ã€‘{title}

ğŸ“ æ‘˜è¦ï¼š
{summary}

ğŸ”— åŸæ–‡é“¾æ¥ï¼š{url}"""
        
        lines.append(article_block)
    
    # æ·»åŠ å¤´éƒ¨ä¿¡æ¯
    header = f"ä»¥ä¸‹ä¸º{formatted_date} The Athletic è¦é—»ç»¼è¿°ï¼Œå…±{len(articles_with_summary)}ç¯‡æ–‡ç« ï¼Œå†…å®¹ç»¼è¿°å¦‚ä¸‹ï¼š\n"
    
    return header + separator + separator.join(lines) + separator


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="æ–‡ç« æ‘˜è¦ç”Ÿæˆå™¨")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‘˜è¦ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰")
    args = parser.parse_args()
    
    # æ£€æŸ¥ API å¯†é’¥
    if not DEEPSEEK_API_KEY:
        print("é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")
        print("ç¤ºä¾‹: DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxx")
        return
    
    # è·å–æ˜¨å¤©çš„æ—¥æœŸ
    date_str = get_yesterday_date()
    
    # æ£€æŸ¥ç›®æ ‡æ‘˜è¦æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    output_file = SUMMARY_DIR / f"{date_str}_summary.txt"
    if output_file.exists() and not args.force:
        print(f"æ‘˜è¦æ–‡ä»¶å·²å­˜åœ¨: {output_file}")
        print(f"å¦‚éœ€é‡æ–°ç”Ÿæˆï¼Œè¯·ä½¿ç”¨ --force å‚æ•°")
        return
    
    print(f"=" * 60)
    print(f"æ–‡ç« æ‘˜è¦ç”Ÿæˆå™¨")
    print(f"=" * 60)
    print(f"å¤„ç†æ—¥æœŸ: {date_str}")
    
    # åŠ è½½æç¤ºè¯
    prompt = load_prompt()
    print(f"âœ“ å·²åŠ è½½æç¤ºè¯: {PROMPT_FILE}")
    
    # åŠ è½½æ–‡ç« 
    articles = load_articles(date_str)
    if not articles:
        print(f"æœªæ‰¾åˆ° {date_str} çš„æ–‡ç« ")
        return
    
    print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    
    # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
    client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL,
    )
    
    # é€ç¯‡ç”Ÿæˆæ‘˜è¦
    print("\nå¼€å§‹ç”Ÿæˆæ‘˜è¦...")
    print("-" * 40)
    
    articles_with_summary = []
    
    total_tokens = 0
    
    for i, article in enumerate(articles, 1):
        title = article.get("title", "æ— æ ‡é¢˜")[:50]
        print(f"[{i}/{len(articles)}] {title}...")
        
        try:
            summary, tokens = generate_summary(client, prompt, article)
            total_tokens += tokens
        except SummaryGenerationError as e:
            print(f"\n" + "!" * 60)
            print(f"âŒ é”™è¯¯: {e}")
            print(f"âŒ æ–‡ç« : {article.get('title', 'æ— æ ‡é¢˜')}")
            print(f"âŒ æµç¨‹ä¸­æ–­ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– API çŠ¶æ€åé‡è¯•")
            print("!" * 60)
            sys.exit(1)
        
        articles_with_summary.append({
            "title": article.get("title", "æ— æ ‡é¢˜"),
            "url": article.get("url", ""),
            "summary": summary,
        })
        
        print(f"  âœ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    
    # ä¿å­˜æ‘˜è¦æ–‡ä»¶
    output_file = SUMMARY_DIR / f"{date_str}_summary.txt"
    output_content = format_summary_output(articles_with_summary, date_str)
    
    # åœ¨æœ«å°¾æ·»åŠ  token æ¶ˆè€—ç»Ÿè®¡
    output_content += f"\n{'â”€' * 60}\n"
    output_content += f"æœ¬æ¬¡é¢„è®¡æ¶ˆè€— Token æ•°ï¼š{total_tokens:,}\n"
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(output_content)
    
    print("\n" + "=" * 60)
    print(f"âœ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ!")
    print(f"  - å¤„ç†æ–‡ç« : {len(articles)} ç¯‡")
    print(f"  - é¢„è®¡æ¶ˆè€— Token: {total_tokens:,}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {output_file.absolute()}")
    print("=" * 60)


if __name__ == "__main__":
    main()
