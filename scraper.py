"""
The Athletic æ–‡ç« çˆ¬è™«
ä½¿ç”¨ Playwright æ¡†æ¶æå– headline æ–‡ç« å†…å®¹

ä½¿ç”¨æ–¹æ³•ï¼š
1. é¦–æ¬¡è¿è¡Œ: python scraper.py --login
   (ä¼šæ‰“å¼€æµè§ˆå™¨è®©ä½ æ‰‹åŠ¨ç™»å½•ï¼Œç™»å½•æˆåŠŸåæŒ‰ Enter ä¿å­˜ Cookie)
2. åç»­è¿è¡Œ: python scraper.py
   (è‡ªåŠ¨ä½¿ç”¨å·²ä¿å­˜çš„ Cookie)

GitHub Actions ä½¿ç”¨æ–¹æ³•ï¼š
- å°† auth_state.json çš„å†…å®¹ä¿å­˜åˆ° GitHub Secrets çš„ AUTH_STATE_JSON
- å·¥ä½œæµä¼šè‡ªåŠ¨ä» Secrets æ¢å¤è®¤è¯çŠ¶æ€
"""

import os
import sys
import json
import re
import time
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright, Page, Browser

# é…ç½®
BASE_URL = "https://www.nytimes.com/athletic"
NEWS_URL = f"{BASE_URL}/news/"
LOGIN_URL = "https://www.nytimes.com/athletic/login"

# Cookie å­˜å‚¨æ–‡ä»¶
COOKIE_FILE = Path("auth_state.json")

# è¾“å‡ºç›®å½•
ARTICLES_DIR = Path("articles")
ARTICLES_DIR.mkdir(exist_ok=True)

# ç´¢å¼•æ–‡ä»¶ï¼ˆç”¨äºå»é‡ï¼Œé¿å…é‡å¤æŠ“å–ï¼‰
INDEX_FILE = ARTICLES_DIR / "index.json"

# é‡è¯•é…ç½®
MAX_RETRIES = 10
RETRY_DELAY = 5  # ç§’


def get_output_dir_by_date(date_str: str = None) -> Path:
    """
    æ ¹æ®æ—¥æœŸè·å–è¾“å‡ºç›®å½•ï¼Œæ ¼å¼ï¼šarticles/YYYYMMDD/
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneæˆ–è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸ
    """
    folder_date = None
    
    if date_str:
        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
        # ä¾‹å¦‚: "Jan. 30, 2026", "2026-01-30", "January 30, 2026" ç­‰
        import re
        
        # å°è¯•æå–å¹´æœˆæ—¥
        # æ ¼å¼1: "Jan. 30, 2026" æˆ– "January 30, 2026"
        month_map = {
            'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',
            'may': '05', 'jun': '06', 'jul': '07', 'aug': '08',
            'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'
        }
        
        # åŒ¹é… "Mon. DD, YYYY" æˆ– "Month DD, YYYY"
        match = re.search(r'([a-zA-Z]{3,9})\.?\s+(\d{1,2}),?\s+(\d{4})', date_str)
        if match:
            month_abbr = match.group(1)[:3].lower()
            day = match.group(2).zfill(2)
            year = match.group(3)
            if month_abbr in month_map:
                folder_date = f"{year}{month_map[month_abbr]}{day}"
        
        # æ ¼å¼2: "YYYY-MM-DD" æˆ– ISO æ ¼å¼
        if not folder_date:
            match = re.search(r'(\d{4})-(\d{2})-(\d{2})', date_str)
            if match:
                folder_date = f"{match.group(1)}{match.group(2)}{match.group(3)}"
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸ
    if not folder_date:
        folder_date = datetime.now().strftime("%Y%m%d")
    
    output_dir = ARTICLES_DIR / folder_date
    output_dir.mkdir(exist_ok=True)
    return output_dir


def load_index() -> dict:
    """åŠ è½½æ–‡ç« ç´¢å¼• {url: æŠ“å–æ—¶é—´æˆ³}"""
    if INDEX_FILE.exists():
        try:
            with open(INDEX_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {}


def save_index(index: dict):
    """ä¿å­˜æ–‡ç« ç´¢å¼•"""
    with open(INDEX_FILE, "w", encoding="utf-8") as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def is_article_scraped(index: dict, url: str) -> bool:
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²æŠ“å–"""
    return url in index


def goto_with_retry(page: Page, url: str, max_retries: int = MAX_RETRIES, **kwargs) -> bool:
    """
    å¸¦é‡è¯•çš„é¡µé¢å¯¼èˆªï¼Œå¤„ç†æœåŠ¡å™¨é”™è¯¯ç­‰
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½é¡µé¢
    """
    for attempt in range(max_retries):
        try:
            response = page.goto(url, **kwargs)
            
            # æ£€æŸ¥ HTTP çŠ¶æ€ç 
            if response and response.status >= 500:
                print(f"  âš  HTTP {response.status} é”™è¯¯ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                time.sleep(RETRY_DELAY)
                continue
            
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(1)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ˜¾ç¤ºæœåŠ¡å™¨é”™è¯¯ï¼ˆåªæ£€æŸ¥é¡µé¢å¼€å¤´éƒ¨åˆ†ï¼Œé¿å…è¯¯åˆ¤ï¼‰
            try:
                # æ£€æŸ¥ title æˆ– h1 æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                title = page.title().lower()
                error_in_title = 'error' in title or 'unavailable' in title
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦å‡ ä¹ä¸ºç©ºï¼ˆé”™è¯¯é¡µé¢é€šå¸¸å†…å®¹å¾ˆå°‘ï¼‰
                body_text = page.locator('body').inner_text(timeout=5000)
                is_error_page = False
                
                # é”™è¯¯é¡µé¢é€šå¸¸å¾ˆçŸ­ï¼Œä¸”åŒ…å«ç‰¹å®šé”™è¯¯ä¿¡æ¯
                if len(body_text) < 500:
                    body_lower = body_text.lower()
                    error_phrases = [
                        'internal server error',
                        'something went wrong',
                        'service unavailable',
                        'bad gateway',
                        'gateway timeout',
                        'server error',
                    ]
                    is_error_page = any(phrase in body_lower for phrase in error_phrases)
                
                if error_in_title or is_error_page:
                    print(f"  âš  é¡µé¢æ˜¾ç¤ºæœåŠ¡å™¨é”™è¯¯ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                    time.sleep(RETRY_DELAY)
                    continue
            except:
                pass  # å¦‚æœæ— æ³•è·å–é¡µé¢æ–‡æœ¬ï¼Œç»§ç»­æ‰§è¡Œ
            
            # æˆåŠŸ
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯æˆ–è¶…æ—¶ï¼Œé‡è¯•
            if 'timeout' in error_msg or 'net::' in error_msg or 'navigation' in error_msg:
                print(f"  âš  åŠ è½½å¤±è´¥: {e}ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise
    
    print(f"  âœ— é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
    return False


def manual_login_and_save_cookie(p) -> bool:
    """
    æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œç„¶åä¿å­˜ Cookie
    """
    print("=" * 60)
    print("æ‰‹åŠ¨ç™»å½•æ¨¡å¼")
    print("=" * 60)
    print("1. æµè§ˆå™¨å°†æ‰“å¼€ç™»å½•é¡µé¢")
    print("2. è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ä½ çš„ä»˜è´¹è´¦æˆ·")
    print("3. ç™»å½•æˆåŠŸåï¼Œå›åˆ°è¿™é‡ŒæŒ‰ Enter é”®ä¿å­˜ Cookie")
    print("=" * 60)
    
    browser = None
    
    # å¯åŠ¨æµè§ˆå™¨
    try:
        browser = p.firefox.launch(headless=False, slow_mo=50)
    except:
        try:
            browser = p.chromium.launch(headless=False, slow_mo=50)
        except:
            browser = p.webkit.launch(headless=False, slow_mo=50)
    
    context = browser.new_context(
        viewport={"width": 1920, "height": 1080},
    )
    page = context.new_page()
    
    try:
        # æ‰“å¼€ç™»å½•é¡µé¢ï¼ˆæ— è¶…æ—¶é™åˆ¶ï¼‰
        page.goto(LOGIN_URL, timeout=0)
        
        print("\n>>> è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")
        print(">>> ç™»å½•æˆåŠŸåï¼ŒæŒ‰ Enter é”®ç»§ç»­...")
        input()
        
        # ä¿å­˜è®¤è¯çŠ¶æ€ï¼ˆåŒ…æ‹¬ cookiesã€localStorage ç­‰ï¼‰
        context.storage_state(path=str(COOKIE_FILE))
        
        print(f"\nâœ“ Cookie å·²ä¿å­˜åˆ° {COOKIE_FILE}")
        print("ä¸‹æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä½¿ç”¨æ­¤ Cookieï¼Œæ— éœ€é‡æ–°ç™»å½•")
        
        return True
        
    except Exception as e:
        print(f"ä¿å­˜ Cookie å¤±è´¥: {e}")
        return False
    finally:
        browser.close()


def has_saved_cookie() -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ Cookie"""
    return COOKIE_FILE.exists()


def get_article_links(page: Page, debug: bool = True) -> list[dict]:
    """
    ä»æ–°é—»é¡µé¢è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
    """
    print(f"æ­£åœ¨è®¿é—®: {NEWS_URL}")
    if not goto_with_retry(page, NEWS_URL, wait_until="networkidle", timeout=300000):
        print("æ— æ³•åŠ è½½æ–°é—»é¡µé¢")
        return []
    time.sleep(3)
    
    # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹
    print("æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
    for _ in range(5):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(2)
    
    # æ»šå›é¡¶éƒ¨
    page.evaluate("window.scrollTo(0, 0)")
    time.sleep(1)
    
    # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
    if debug:
        html_content = page.content()
        debug_file = ARTICLES_DIR / "debug_page.html"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"âœ“ é¡µé¢HTMLå·²ä¿å­˜åˆ° {debug_file}")
    
    articles = []
    seen_urls = set()
    
    # æ–‡ç« URLæ ¼å¼ï¼š/athletic/æ•°å­—ID/æ—¥æœŸ/æ ‡é¢˜/
    article_url_pattern = re.compile(r'nytimes\.com/athletic/\d+/\d{4}/\d{2}/\d{2}/')
    
    all_links = page.locator('a[href*="nytimes.com/athletic/"]').all()
    print(f"  æ‰¾åˆ° {len(all_links)} ä¸ª Athletic é“¾æ¥")
    
    for link in all_links:
        try:
            href = link.get_attribute("href")
            if not href or href in seen_urls:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥ï¼ˆåŒ…å«æ•°å­—IDå’Œæ—¥æœŸï¼‰
            if not article_url_pattern.search(href):
                continue
            
            # æ’é™¤éæ–‡ç« é¡µé¢
            exclude_patterns = ["/login", "/subscribe", "/account", "/author/", "/team/", "/league/", "/podcast/"]
            if any(pattern in href for pattern in exclude_patterns):
                continue
            
            # è·å–æ ‡é¢˜ - ä¼˜å…ˆä» h5 æ ‡é¢˜å…ƒç´ è·å–
            title = ""
            try:
                # å°è¯•åœ¨é“¾æ¥å†…éƒ¨æ‰¾æ ‡é¢˜
                headline = link.locator('h5, h4, h3, h2, h1').first
                if headline.count() > 0:
                    title = headline.inner_text().strip()
            except:
                pass
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é“¾æ¥æ–‡æœ¬
            if not title:
                title = link.inner_text().strip()
                # æ¸…ç†æ ‡é¢˜ï¼ˆå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼‰
                lines = [l.strip() for l in title.split('\n') if l.strip() and len(l.strip()) > 10]
                title = lines[0] if lines else ""
            
            if title and len(title) > 10:
                seen_urls.add(href)
                articles.append({
                    "title": title[:200],
                    "url": href
                })
                print(f"    âœ“ {title[:60]}...")
        except Exception as e:
            continue
    
    print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def extract_article_content(page: Page, url: str, save_html: bool = False) -> dict:
    """
    æå–å•ç¯‡æ–‡ç« çš„å†…å®¹
    
    Args:
        save_html: å¦‚æœä¸º Trueï¼Œä¿å­˜æ–‡ç« HTMLåˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
    """
    try:
        # å¸¦é‡è¯•çš„é¡µé¢åŠ è½½
        if not goto_with_retry(page, url, wait_until="domcontentloaded", timeout=300000):
            return {
                "url": url,
                "error": "é¡µé¢åŠ è½½å¤±è´¥ï¼ˆå¤šæ¬¡é‡è¯•åï¼‰",
                "scraped_at": datetime.now().isoformat(),
            }
        time.sleep(3)
        
        # ä¿å­˜HTMLç”¨äºè°ƒè¯•ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œç¡®ä¿èƒ½ä¿å­˜ï¼‰
        if save_html:
            try:
                html_content = page.content()
                debug_file = ARTICLES_DIR / "debug_article.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"  âœ“ æ–‡ç« HTMLå·²ä¿å­˜åˆ° {debug_file}")
            except Exception as e:
                print(f"  âœ— ä¿å­˜HTMLå¤±è´¥: {e}")
        
        article_data = {
            "url": url,
            "scraped_at": datetime.now().isoformat(),
        }
        
        # æå–æ ‡é¢˜
        title_selectors = [
            'h1',
            'article h1',
            '[data-testid="headline"]',
            '.headline',
            '.article-title',
        ]
        
        for selector in title_selectors:
            try:
                title_el = page.locator(selector).first
                if title_el.is_visible():
                    article_data["title"] = title_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–ä½œè€…
        author_selectors = [
            '[data-testid="byline"]',
            '.byline',
            '.author',
            'a[href*="/author/"]',
        ]
        
        for selector in author_selectors:
            try:
                author_el = page.locator(selector).first
                if author_el.is_visible():
                    article_data["author"] = author_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–å‘å¸ƒæ—¥æœŸ
        date_selectors = [
            'time',
            '[data-testid="timestamp"]',
            '.publish-date',
            '.date',
        ]
        
        for selector in date_selectors:
            try:
                date_el = page.locator(selector).first
                if date_el.is_visible():
                    article_data["published_date"] = date_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–æ­£æ–‡å†…å®¹
        # The Athletic çš„æ­£æ–‡åœ¨ .article-content-container é‡Œé¢çš„ <p> æ ‡ç­¾
        # éœ€è¦æ’é™¤ï¼šå›¾ç‰‡ç‰ˆæƒã€å¹¿å‘Šã€æ¨èå†…å®¹ç­‰
        paragraphs = []
        
        # ç›´æ¥é€‰æ‹©æ­£æ–‡å®¹å™¨å†…çš„ p æ ‡ç­¾ï¼Œä½†æ’é™¤ç‰¹å®šç±»
        # æ­£æ–‡ p æ ‡ç­¾æ²¡æœ‰ç‰¹æ®Š classï¼Œè€Œå›¾ç‰‡ç‰ˆæƒç­‰æœ‰ç‰¹å®š class
        content_selector = 'div.article-content-container > p:not([class])'
        p_elements = page.locator(content_selector).all()
        
        print(f"  æ‰¾åˆ° {len(p_elements)} ä¸ªæ­£æ–‡æ®µè½ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {content_selector}ï¼‰")
        
        if p_elements:
            for p in p_elements:
                try:
                    text = p.inner_text().strip()
                    if text and len(text) > 10:
                        paragraphs.append(text)
                except:
                    continue
        
        # å¦‚æœç›´æ¥å­å…ƒç´ æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„é€‰æ‹©å™¨
        if not paragraphs:
            # è·å–å®¹å™¨å†…æ‰€æœ‰ pï¼Œä½†æ’é™¤ ignore å’Œ ad å†…çš„
            content_container = page.locator('div.article-content-container').first
            if content_container.count() > 0:
                all_p = content_container.locator('p').all()
                print(f"  å¤‡ç”¨ï¼šæ‰¾åˆ° {len(all_p)} ä¸ª p æ ‡ç­¾")
                
                for p in all_p:
                    try:
                        # è·å– p çš„ class å±æ€§
                        p_class = p.get_attribute('class') or ''
                        
                        # æ’é™¤æœ‰ç‰¹å®š class çš„ pï¼ˆå›¾ç‰‡ç‰ˆæƒã€å¹¿å‘Šç­‰ï¼‰
                        skip_classes = ['ImageCaption', 'ImageCredit', 'ad-slug', 'showcase']
                        if any(skip in p_class for skip in skip_classes):
                            continue
                        
                        text = p.inner_text().strip()
                        
                        # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½å’Œå¹¿å‘Šç›¸å…³æ–‡æœ¬
                        if text and len(text) > 20:
                            # æ’é™¤å¸¸è§çš„éæ­£æ–‡å†…å®¹
                            skip_texts = ['advertisement', 'follow', 'twitter', '@', 'getty images', 'photo:']
                            if not any(skip.lower() in text.lower()[:50] for skip in skip_texts):
                                paragraphs.append(text)
                    except:
                        continue
        
        article_data["content"] = "\n\n".join(paragraphs)
        article_data["paragraph_count"] = len(paragraphs)
        
        return article_data
        
    except Exception as e:
        return {
            "url": url,
            "error": str(e),
            "scraped_at": datetime.now().isoformat(),
        }


def save_article(article: dict, output_dir: Path) -> Path:
    """
    ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶
    
    Args:
        article: æ–‡ç« æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶åï¼šä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    title = article.get("title", "untitled")
    safe_title = "".join(c if c.isalnum() or c == "_" else "_" if c == " " else "" for c in title)
    # åˆå¹¶è¿ç»­çš„ä¸‹åˆ’çº¿ï¼Œå¹¶é™åˆ¶é•¿åº¦
    safe_title = "_".join(part for part in safe_title.split("_") if part)[:80]
    filename = f"{safe_title}.json"
    
    filepath = output_dir / filename
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(article, f, ensure_ascii=False, indent=2)
    
    return filepath


def is_ci_environment() -> bool:
    """æ£€æµ‹æ˜¯å¦åœ¨ CI ç¯å¢ƒä¸­è¿è¡Œ"""
    return os.environ.get("CI") == "true" or os.environ.get("GITHUB_ACTIONS") == "true"


def launch_browser(p, with_cookie: bool = False):
    """
    å¯åŠ¨æµè§ˆå™¨ï¼Œå¯é€‰æ‹©æ€§åœ°åŠ è½½å·²ä¿å­˜çš„ Cookie
    åœ¨ CI ç¯å¢ƒä¸­è‡ªåŠ¨ä½¿ç”¨ headless æ¨¡å¼
    """
    browser = None
    is_ci = is_ci_environment()
    headless = is_ci  # CI ç¯å¢ƒä½¿ç”¨ headless æ¨¡å¼
    
    if is_ci:
        print("ğŸ¤– æ£€æµ‹åˆ° CI ç¯å¢ƒï¼Œä½¿ç”¨ headless æ¨¡å¼")
    
    # åœ¨ CI ç¯å¢ƒä¸­ä¼˜å…ˆä½¿ç”¨ Chromiumï¼ˆå·²å®‰è£…ï¼‰
    browser_order = [(p.chromium, "Chromium")] if is_ci else [
        (p.firefox, "Firefox"), 
        (p.webkit, "WebKit"), 
        (p.chromium, "Chromium")
    ]
    
    # å°è¯•å¯åŠ¨æµè§ˆå™¨
    for browser_type, name in browser_order:
        try:
            print(f"å°è¯•å¯åŠ¨ {name} æµè§ˆå™¨...")
            if name == "Chromium":
                browser = browser_type.launch(
                    headless=headless,
                    slow_mo=50 if is_ci else 100,
                    args=['--disable-gpu', '--no-sandbox', '--disable-dev-shm-usage']
                )
            else:
                browser = browser_type.launch(headless=headless, slow_mo=50 if is_ci else 100)
            print(f"âœ“ {name} å¯åŠ¨æˆåŠŸ")
            break
        except Exception as e:
            print(f"{name} å¯åŠ¨å¤±è´¥: {e}")
    
    if browser is None:
        print("æ‰€æœ‰æµè§ˆå™¨éƒ½æ— æ³•å¯åŠ¨ï¼Œè¯·è¿è¡Œ: playwright install chromium")
        return None, None
    
    # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œå¯é€‰æ‹©æ€§åœ°åŠ è½½ Cookie
    context_options = {
        "viewport": {"width": 1920, "height": 1080},
        "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    if with_cookie and COOKIE_FILE.exists():
        context_options["storage_state"] = str(COOKIE_FILE)
        print(f"âœ“ å·²åŠ è½½ Cookie: {COOKIE_FILE}")
    
    context = browser.new_context(**context_options)
    return browser, context


def main():
    """
    ä¸»å‡½æ•°
    
    ä½¿ç”¨æ–¹æ³•:
      python scraper.py --login   # æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie
      python scraper.py           # ä½¿ç”¨å·²ä¿å­˜çš„ Cookie è¿›è¡Œçˆ¬å–
    """
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    login_mode = "--login" in sys.argv
    debug_mode = "--debug" in sys.argv  # è°ƒè¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€ç¯‡æ–‡ç« 
    save_html = "--save-html" in sys.argv  # ä¿å­˜HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•
    
    print("=" * 60)
    print("The Athletic æ–‡ç« çˆ¬è™«")
    print("=" * 60)
    
    with sync_playwright() as p:
        
        # ç™»å½•æ¨¡å¼ï¼šæ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie
        if login_mode:
            manual_login_and_save_cookie(p)
            return
        
        # çˆ¬å–æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ Cookie
        if not has_saved_cookie():
            print("âŒ æœªæ‰¾åˆ°å·²ä¿å­˜çš„ Cookie")
            print("")
            print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œç™»å½•ï¼š")
            print("  python scraper.py --login")
            print("")
            return
        
        # å¯åŠ¨æµè§ˆå™¨å¹¶åŠ è½½ Cookie
        browser, context = launch_browser(p, with_cookie=True)
        if browser is None:
            return
        
        page = context.new_page()
        
        try:
            # åŠ è½½æ–‡ç« ç´¢å¼•ï¼ˆç”¨äºå»é‡ï¼‰
            index = load_index()
            print(f"âœ“ å·²åŠ è½½ç´¢å¼•ï¼Œå†å²æŠ“å–æ–‡ç« æ•°: {len(index)}")
            
            # è·å–æ–‡ç« é“¾æ¥
            articles = get_article_links(page, debug=save_html)
            
            if not articles:
                print("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
                return
            
            # è¿‡æ»¤å·²æŠ“å–çš„æ–‡ç« 
            new_articles = []
            skipped_count = 0
            for article in articles:
                if is_article_scraped(index, article["url"]):
                    skipped_count += 1
                else:
                    new_articles.append(article)
            
            print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {skipped_count} ç¯‡å·²æŠ“å–ï¼Œ{len(new_articles)} ç¯‡å¾…æŠ“å–")
            
            if not new_articles:
                print("æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æŠ“å–")
                return
            
            # æå–æ¯ç¯‡æ–‡ç« å†…å®¹
            print("\nå¼€å§‹æå–æ–‡ç« å†…å®¹...")
            print("-" * 40)
            
            # è°ƒè¯•æ¨¡å¼åªæŠ“å–ç¬¬ä¸€ç¯‡
            if debug_mode:
                print("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€ç¯‡æ–‡ç« ")
                new_articles = new_articles[:1]
            
            all_articles = []
            success_count = 0
            
            for i, article_info in enumerate(new_articles, 1):
                print(f"[{i}/{len(new_articles)}] æ­£åœ¨æå–: {article_info['title'][:50]}...")
                
                # åªåœ¨æŒ‡å®š --save-html å‚æ•°ä¸”æ˜¯ç¬¬ä¸€ç¯‡æ–‡ç« æ—¶ä¿å­˜HTML
                should_save_html = save_html and (i == 1)
                article_data = extract_article_content(page, article_info["url"], save_html=should_save_html)
                all_articles.append(article_data)
                
                content_len = len(article_data.get("content", ""))
                if "error" in article_data:
                    print(f"  âœ— æå–å¤±è´¥: {article_data['error']}")
                else:
                    # æ ¹æ®å‘å¸ƒæ—¥æœŸç¡®å®šå­˜å‚¨ç›®å½•
                    published_date = article_data.get("published_date", "")
                    output_dir = get_output_dir_by_date(published_date)
                    
                    # ä¿å­˜å•ç¯‡æ–‡ç« 
                    filepath = save_article(article_data, output_dir)
                    
                    print(f"  âœ“ å·²ä¿å­˜: {output_dir.name}/{filepath.name} ({content_len} å­—ç¬¦)")
                    success_count += 1
                    
                    # æ›´æ–°ç´¢å¼•
                    index[article_info["url"]] = datetime.now().isoformat()
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)
            
            # ä¿å­˜ç´¢å¼•
            save_index(index)
            print(f"âœ“ ç´¢å¼•å·²æ›´æ–°ï¼Œå½“å‰æ€»æ–‡ç« æ•°: {len(index)}")
            
            print("\n" + "=" * 60)
            print(f"âœ“ çˆ¬å–å®Œæˆ!")
            print(f"  - æœ¬æ¬¡æŠ“å–: {len(new_articles)} ç¯‡")
            print(f"  - æˆåŠŸæå–: {success_count} ç¯‡")
            print(f"  - è·³è¿‡å·²æŠ“å–: {skipped_count} ç¯‡")
            print(f"  - è¾“å‡ºç›®å½•: {ARTICLES_DIR.absolute()} (æŒ‰å‘å¸ƒæ—¥æœŸåˆ†ç›®å½•)")
            print(f"  - ç´¢å¼•æ–‡ä»¶: {INDEX_FILE.absolute()}")
            print("=" * 60)
            
        finally:
            browser.close()


if __name__ == "__main__":
    main()

