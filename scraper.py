"""
The Athletic æ–‡ç« çˆ¬è™«
ä½¿ç”¨ Playwright æ¡†æ¶æå– headline æ–‡ç« å†…å®¹

ä½¿ç”¨æ–¹æ³•ï¼š
1. é¦–æ¬¡è¿è¡Œ: python scraper.py --login
   (ä¼šæ‰“å¼€æµè§ˆå™¨è®©ä½ æ‰‹åŠ¨ç™»å½•ï¼Œç™»å½•æˆåŠŸåæŒ‰ Enter ä¿å­˜ Cookie)
2. åç»­è¿è¡Œ: python scraper.py
   (è‡ªåŠ¨ä½¿ç”¨å·²ä¿å­˜çš„ Cookie)

GitHub Actions ä½¿ç”¨æ–¹æ³•ï¼š
- å°† auth_state.json çš„å†…å®¹ä¿å­˜åˆ° GitHub Secrets çš„ AUTH_STATE_JSON
- å·¥ä½œæµä¼šè‡ªåŠ¨ä» Secrets æ¢å¤è®¤è¯çŠ¶æ€
"""

import os
import sys

# åœ¨ CI ç¯å¢ƒä¸­ç¦ç”¨è¾“å‡ºç¼“å†²
if os.environ.get("CI") == "true" or os.environ.get("GITHUB_ACTIONS") == "true":
    # å¼ºåˆ¶ä½¿ç”¨æ— ç¼“å†²è¾“å‡º
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)

print("[DEBUG] è„šæœ¬å¼€å§‹æ‰§è¡Œ...", flush=True)

import json
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

# æ—¶åŒºå¤„ç†
try:
    from zoneinfo import ZoneInfo
except ImportError:
    # Python < 3.9 ä½¿ç”¨ pytz
    try:
        import pytz
        def ZoneInfo(name):
            return pytz.timezone(name)
    except ImportError:
        raise ImportError("éœ€è¦ zoneinfo (Python 3.9+) æˆ– pytz åº“æ¥å¤„ç†æ—¶åŒº")

print("[DEBUG] åŸºç¡€æ¨¡å—å¯¼å…¥å®Œæˆ", flush=True)

from playwright.sync_api import sync_playwright, Page, Browser

print("[DEBUG] Playwright å¯¼å…¥å®Œæˆ", flush=True)

# é…ç½®
BASE_URL = "https://www.nytimes.com/athletic"
NEWS_URL = f"{BASE_URL}/news/"
LOGIN_URL = "https://www.nytimes.com/athletic/login"

# Cookie å­˜å‚¨æ–‡ä»¶
COOKIE_FILE = Path("auth_state.json")

# è¾“å‡ºç›®å½•
ARTICLES_DIR = Path("articles")
ARTICLES_DIR.mkdir(exist_ok=True)

# ç´¢å¼•æ–‡ä»¶ï¼ˆç”¨äºå»é‡ï¼Œé¿å…é‡å¤æŠ“å–ï¼‰
INDEX_FILE = ARTICLES_DIR / "index.json"

# é‡è¯•é…ç½®
MAX_RETRIES = 10
RETRY_DELAY = 5  # ç§’


def get_output_dir_by_date(date_str: str = None) -> Path:
    """
    æ ¹æ®æ—¥æœŸè·å–è¾“å‡ºç›®å½•ï¼Œæ ¼å¼ï¼šarticles/YYYYMMDD/
    æ—¥æœŸæŒ‰ç…§è‹±å›½ä¼¦æ•¦æ—¶é—´ï¼ˆGMT/BSTï¼‰æ¥åˆ’åˆ†
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "Feb. 6, 2026Updated 12:17 am GMT+8"
                  å¦‚æœä¸ºNoneæˆ–è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸï¼ˆä¼¦æ•¦æ—¶é—´ï¼‰
    """
    folder_date = None
    london_tz = ZoneInfo("Europe/London")
    
    if date_str:
        # å°è¯•è§£ææ—¥æœŸã€æ—¶é—´å’Œæ—¶åŒºä¿¡æ¯
        month_map = {
            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,
            'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,
            'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
        }
        
        # è§£ææ ¼å¼: "Feb. 6, 2026Updated 12:17 am GMT+8"
        # æˆ–ç±»ä¼¼æ ¼å¼: "Jan. 30, 2026 3:45 pm GMT-5"
        
        # 1. æå–æ—¥æœŸéƒ¨åˆ†: "Feb. 6, 2026" æˆ– "January 30, 2026"
        date_match = re.search(r'([a-zA-Z]{3,9})\.?\s+(\d{1,2}),?\s+(\d{4})', date_str)
        
        # 2. æå–æ—¶é—´éƒ¨åˆ†: "12:17 am" æˆ– "3:45 pm"
        time_match = re.search(r'(\d{1,2}):(\d{2})\s*(am|pm)', date_str, re.IGNORECASE)
        
        # 3. æå–æ—¶åŒºéƒ¨åˆ†: "GMT+8", "GMT-5", "UTC+8" ç­‰
        tz_match = re.search(r'(GMT|UTC)([+-])(\d+)', date_str, re.IGNORECASE)
        
        if date_match:
            month_abbr = date_match.group(1)[:3].lower()
            day = int(date_match.group(2))
            year = int(date_match.group(3))
            
            if month_abbr in month_map:
                month = month_map[month_abbr]
                
                # è§£ææ—¶é—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                hour = 0
                minute = 0
                if time_match:
                    hour = int(time_match.group(1))
                    minute = int(time_match.group(2))
                    am_pm = time_match.group(3).lower()
                    
                    # è½¬æ¢ä¸º24å°æ—¶åˆ¶
                    if am_pm == 'pm' and hour != 12:
                        hour += 12
                    elif am_pm == 'am' and hour == 12:
                        hour = 0
                
                # è§£ææ—¶åŒºåç§»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                tz_offset_hours = 0
                if tz_match:
                    sign = tz_match.group(2)
                    offset = int(tz_match.group(3))
                    tz_offset_hours = offset if sign == '+' else -offset
                
                try:
                    # åˆ›å»ºå¸¦æ—¶åŒºçš„ datetime å¯¹è±¡
                    # å‡è®¾åŸå§‹æ—¶é—´æ˜¯ GMT+offset æ—¶åŒº
                    
                    # åˆ›å»ºåŸå§‹æ—¶é—´çš„ datetimeï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼‰
                    dt_naive = datetime(year, month, day, hour, minute)
                    
                    # è®¡ç®— UTC æ—¶é—´ï¼ˆGMT = UTCï¼‰
                    # å¦‚æœåŸå§‹æ—¶é—´æ˜¯ GMT+8ï¼Œé‚£ä¹ˆ UTC = åŸå§‹æ—¶é—´ - 8å°æ—¶
                    dt_utc = dt_naive - timedelta(hours=tz_offset_hours)
                    
                    # è½¬æ¢ä¸ºä¼¦æ•¦æ—¶é—´
                    dt_utc = dt_utc.replace(tzinfo=ZoneInfo("UTC"))
                    dt_london = dt_utc.astimezone(london_tz)
                    
                    # æ ¹æ®ä¼¦æ•¦æ—¶é—´ç¡®å®šæ–‡ä»¶å¤¹æ—¥æœŸ
                    folder_date = dt_london.strftime("%Y%m%d")
                except Exception as e:
                    print(f"  è­¦å‘Š: è§£ææ—¥æœŸæ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸ")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸ
                    folder_date = f"{year}{month:02d}{day:02d}"
        
        # å¦‚æœä¸Šé¢çš„è§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æ—¥æœŸæ ¼å¼: "YYYY-MM-DD"
        if not folder_date:
            match = re.search(r'(\d{4})-(\d{2})-(\d{2})', date_str)
            if match:
                # å¯¹äºç®€å•æ ¼å¼ï¼Œå‡è®¾æ˜¯ UTC æ—¶é—´ï¼Œè½¬æ¢ä¸ºä¼¦æ•¦æ—¶é—´
                try:
                    year = int(match.group(1))
                    month = int(match.group(2))
                    day = int(match.group(3))
                    dt_utc = datetime(year, month, day, tzinfo=ZoneInfo("UTC"))
                    dt_london = dt_utc.astimezone(london_tz)
                    folder_date = dt_london.strftime("%Y%m%d")
                except:
                    folder_date = f"{match.group(1)}{match.group(2)}{match.group(3)}"
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸï¼ˆä¼¦æ•¦æ—¶é—´ï¼‰
    if not folder_date:
        now_london = datetime.now(london_tz)
        folder_date = now_london.strftime("%Y%m%d")
    
    output_dir = ARTICLES_DIR / folder_date
    output_dir.mkdir(exist_ok=True)
    return output_dir


def load_index() -> dict:
    """åŠ è½½æ–‡ç« ç´¢å¼• {url: æŠ“å–æ—¶é—´æˆ³}"""
    if INDEX_FILE.exists():
        try:
            with open(INDEX_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {}


def save_index(index: dict):
    """ä¿å­˜æ–‡ç« ç´¢å¼•"""
    with open(INDEX_FILE, "w", encoding="utf-8") as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def is_article_scraped(index: dict, url: str) -> bool:
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²æŠ“å–"""
    return url in index


def goto_with_retry(page: Page, url: str, max_retries: int = MAX_RETRIES, **kwargs) -> bool:
    """
    å¸¦é‡è¯•çš„é¡µé¢å¯¼èˆªï¼Œå¤„ç†æœåŠ¡å™¨é”™è¯¯ç­‰
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½é¡µé¢
    """
    for attempt in range(max_retries):
        try:
            response = page.goto(url, **kwargs)
            
            # æ£€æŸ¥ HTTP çŠ¶æ€ç 
            if response and response.status >= 500:
                print(f"  âš  HTTP {response.status} é”™è¯¯ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                time.sleep(RETRY_DELAY)
                continue
            
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(1)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ˜¾ç¤ºæœåŠ¡å™¨é”™è¯¯ï¼ˆåªæ£€æŸ¥é¡µé¢å¼€å¤´éƒ¨åˆ†ï¼Œé¿å…è¯¯åˆ¤ï¼‰
            try:
                # æ£€æŸ¥ title æˆ– h1 æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                title = page.title().lower()
                error_in_title = 'error' in title or 'unavailable' in title
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦å‡ ä¹ä¸ºç©ºï¼ˆé”™è¯¯é¡µé¢é€šå¸¸å†…å®¹å¾ˆå°‘ï¼‰
                body_text = page.locator('body').inner_text(timeout=5000)
                is_error_page = False
                
                # é”™è¯¯é¡µé¢é€šå¸¸å¾ˆçŸ­ï¼Œä¸”åŒ…å«ç‰¹å®šé”™è¯¯ä¿¡æ¯
                if len(body_text) < 500:
                    body_lower = body_text.lower()
                    error_phrases = [
                        'internal server error',
                        'something went wrong',
                        'service unavailable',
                        'bad gateway',
                        'gateway timeout',
                        'server error',
                    ]
                    is_error_page = any(phrase in body_lower for phrase in error_phrases)
                
                if error_in_title or is_error_page:
                    print(f"  âš  é¡µé¢æ˜¾ç¤ºæœåŠ¡å™¨é”™è¯¯ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                    time.sleep(RETRY_DELAY)
                    continue
            except:
                pass  # å¦‚æœæ— æ³•è·å–é¡µé¢æ–‡æœ¬ï¼Œç»§ç»­æ‰§è¡Œ
            
            # æˆåŠŸ
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯æˆ–è¶…æ—¶ï¼Œé‡è¯•
            if 'timeout' in error_msg or 'net::' in error_msg or 'navigation' in error_msg:
                print(f"  âš  åŠ è½½å¤±è´¥: {e}ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise
    
    print(f"  âœ— é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
    return False


def manual_login_and_save_cookie(p) -> bool:
    """
    æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œç„¶åä¿å­˜ Cookie
    """
    print("=" * 60)
    print("æ‰‹åŠ¨ç™»å½•æ¨¡å¼")
    print("=" * 60)
    print("1. æµè§ˆå™¨å°†æ‰“å¼€ç™»å½•é¡µé¢")
    print("2. è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ä½ çš„ä»˜è´¹è´¦æˆ·")
    print("3. ç™»å½•æˆåŠŸåï¼Œå›åˆ°è¿™é‡ŒæŒ‰ Enter é”®ä¿å­˜ Cookie")
    print("=" * 60)
    
    browser = None
    
    # å¯åŠ¨æµè§ˆå™¨
    try:
        browser = p.firefox.launch(headless=False, slow_mo=50)
    except:
        try:
            browser = p.chromium.launch(headless=False, slow_mo=50)
        except:
            browser = p.webkit.launch(headless=False, slow_mo=50)
    
    context = browser.new_context(
        viewport={"width": 1920, "height": 1080},
    )
    page = context.new_page()
    
    try:
        # æ‰“å¼€ç™»å½•é¡µé¢ï¼ˆæ— è¶…æ—¶é™åˆ¶ï¼‰
        page.goto(LOGIN_URL, timeout=0)
        
        print("\n>>> è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")
        print(">>> ç™»å½•æˆåŠŸåï¼ŒæŒ‰ Enter é”®ç»§ç»­...")
        input()
        
        # ä¿å­˜è®¤è¯çŠ¶æ€ï¼ˆåŒ…æ‹¬ cookiesã€localStorage ç­‰ï¼‰
        context.storage_state(path=str(COOKIE_FILE))
        
        print(f"\nâœ“ Cookie å·²ä¿å­˜åˆ° {COOKIE_FILE}")
        print("ä¸‹æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä½¿ç”¨æ­¤ Cookieï¼Œæ— éœ€é‡æ–°ç™»å½•")
        
        return True
        
    except Exception as e:
        print(f"ä¿å­˜ Cookie å¤±è´¥: {e}")
        return False
    finally:
        browser.close()


def has_saved_cookie() -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ Cookie"""
    return COOKIE_FILE.exists()


def get_article_links(page: Page, debug: bool = True) -> list[dict]:
    """
    ä»æ–°é—»é¡µé¢è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
    """
    print(f"æ­£åœ¨è®¿é—®: {NEWS_URL}", flush=True)
    # ä½¿ç”¨ domcontentloaded è€Œä¸æ˜¯ networkidleï¼Œé¿å…åœ¨ CI ç¯å¢ƒä¸­æ— é™ç­‰å¾…
    if not goto_with_retry(page, NEWS_URL, wait_until="domcontentloaded", timeout=120000):
        print("æ— æ³•åŠ è½½æ–°é—»é¡µé¢", flush=True)
        return []
    print("âœ“ é¡µé¢ DOM åŠ è½½å®Œæˆ", flush=True)
    time.sleep(3)
    
    # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹
    print("æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...", flush=True)
    for i in range(5):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        print(f"  æ»šåŠ¨ {i+1}/5", flush=True)
        time.sleep(2)
    
    # æ»šå›é¡¶éƒ¨
    page.evaluate("window.scrollTo(0, 0)")
    time.sleep(1)
    
    # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
    if debug:
        html_content = page.content()
        debug_file = ARTICLES_DIR / "debug_page.html"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"âœ“ é¡µé¢HTMLå·²ä¿å­˜åˆ° {debug_file}")
    
    articles = []
    seen_urls = set()
    
    # æ–‡ç« URLæ ¼å¼ï¼š/athletic/æ•°å­—ID/æ—¥æœŸ/æ ‡é¢˜/
    article_url_pattern = re.compile(r'nytimes\.com/athletic/\d+/\d{4}/\d{2}/\d{2}/')
    
    all_links = page.locator('a[href*="nytimes.com/athletic/"]').all()
    print(f"  æ‰¾åˆ° {len(all_links)} ä¸ª Athletic é“¾æ¥")
    
    for link in all_links:
        try:
            href = link.get_attribute("href")
            if not href or href in seen_urls:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥ï¼ˆåŒ…å«æ•°å­—IDå’Œæ—¥æœŸï¼‰
            if not article_url_pattern.search(href):
                continue
            
            # æ’é™¤éæ–‡ç« é¡µé¢
            exclude_patterns = ["/login", "/subscribe", "/account", "/author/", "/team/", "/league/", "/podcast/"]
            if any(pattern in href for pattern in exclude_patterns):
                continue
            
            # è·å–æ ‡é¢˜ - ä¼˜å…ˆä» h5 æ ‡é¢˜å…ƒç´ è·å–
            title = ""
            try:
                # å°è¯•åœ¨é“¾æ¥å†…éƒ¨æ‰¾æ ‡é¢˜
                headline = link.locator('h5, h4, h3, h2, h1').first
                if headline.count() > 0:
                    title = headline.inner_text().strip()
            except:
                pass
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é“¾æ¥æ–‡æœ¬
            if not title:
                title = link.inner_text().strip()
                # æ¸…ç†æ ‡é¢˜ï¼ˆå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼‰
                lines = [l.strip() for l in title.split('\n') if l.strip() and len(l.strip()) > 10]
                title = lines[0] if lines else ""
            
            if title and len(title) > 10:
                seen_urls.add(href)
                articles.append({
                    "title": title[:200],
                    "url": href
                })
                print(f"    âœ“ {title[:60]}...")
        except Exception as e:
            continue
    
    print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def extract_article_content(page: Page, url: str, save_html: bool = False) -> dict:
    """
    æå–å•ç¯‡æ–‡ç« çš„å†…å®¹
    
    Args:
        save_html: å¦‚æœä¸º Trueï¼Œä¿å­˜æ–‡ç« HTMLåˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
    """
    try:
        # å¸¦é‡è¯•çš„é¡µé¢åŠ è½½
        if not goto_with_retry(page, url, wait_until="domcontentloaded", timeout=120000):
            return {
                "url": url,
                "error": "é¡µé¢åŠ è½½å¤±è´¥ï¼ˆå¤šæ¬¡é‡è¯•åï¼‰",
                "scraped_at": datetime.now().isoformat(),
            }
        time.sleep(3)
        
        # ä¿å­˜HTMLç”¨äºè°ƒè¯•ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œç¡®ä¿èƒ½ä¿å­˜ï¼‰
        if save_html:
            try:
                html_content = page.content()
                debug_file = ARTICLES_DIR / "debug_article.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"  âœ“ æ–‡ç« HTMLå·²ä¿å­˜åˆ° {debug_file}")
            except Exception as e:
                print(f"  âœ— ä¿å­˜HTMLå¤±è´¥: {e}")
        
        article_data = {
            "url": url,
            "scraped_at": datetime.now().isoformat(),
        }
        
        # æå–æ ‡é¢˜
        title_selectors = [
            'h1',
            'article h1',
            '[data-testid="headline"]',
            '.headline',
            '.article-title',
        ]
        
        for selector in title_selectors:
            try:
                title_el = page.locator(selector).first
                if title_el.is_visible():
                    article_data["title"] = title_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–ä½œè€…
        author_selectors = [
            '[data-testid="byline"]',
            '.byline',
            '.author',
            'a[href*="/author/"]',
        ]
        
        for selector in author_selectors:
            try:
                author_el = page.locator(selector).first
                if author_el.is_visible():
                    article_data["author"] = author_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–å‘å¸ƒæ—¥æœŸ
        date_selectors = [
            'time',
            '[data-testid="timestamp"]',
            '.publish-date',
            '.date',
        ]
        
        for selector in date_selectors:
            try:
                date_el = page.locator(selector).first
                if date_el.is_visible():
                    article_data["published_date"] = date_el.inner_text().strip()
                    break
            except:
                continue
        
        # æå–æ­£æ–‡å†…å®¹
        # The Athletic çš„æ­£æ–‡åœ¨ .article-content-container é‡Œé¢çš„ <p> æ ‡ç­¾
        # éœ€è¦æ’é™¤ï¼šå›¾ç‰‡ç‰ˆæƒã€å¹¿å‘Šã€æ¨èå†…å®¹ç­‰
        paragraphs = []
        
        # ç›´æ¥é€‰æ‹©æ­£æ–‡å®¹å™¨å†…çš„ p æ ‡ç­¾ï¼Œä½†æ’é™¤ç‰¹å®šç±»
        # æ­£æ–‡ p æ ‡ç­¾æ²¡æœ‰ç‰¹æ®Š classï¼Œè€Œå›¾ç‰‡ç‰ˆæƒç­‰æœ‰ç‰¹å®š class
        content_selector = 'div.article-content-container > p:not([class])'
        p_elements = page.locator(content_selector).all()
        
        print(f"  æ‰¾åˆ° {len(p_elements)} ä¸ªæ­£æ–‡æ®µè½ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {content_selector}ï¼‰")
        
        if p_elements:
            for p in p_elements:
                try:
                    text = p.inner_text().strip()
                    if text and len(text) > 10:
                        paragraphs.append(text)
                except:
                    continue
        
        # å¦‚æœç›´æ¥å­å…ƒç´ æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„é€‰æ‹©å™¨
        if not paragraphs:
            # è·å–å®¹å™¨å†…æ‰€æœ‰ pï¼Œä½†æ’é™¤ ignore å’Œ ad å†…çš„
            content_container = page.locator('div.article-content-container').first
            if content_container.count() > 0:
                all_p = content_container.locator('p').all()
                print(f"  å¤‡ç”¨ï¼šæ‰¾åˆ° {len(all_p)} ä¸ª p æ ‡ç­¾")
                
                for p in all_p:
                    try:
                        # è·å– p çš„ class å±æ€§
                        p_class = p.get_attribute('class') or ''
                        
                        # æ’é™¤æœ‰ç‰¹å®š class çš„ pï¼ˆå›¾ç‰‡ç‰ˆæƒã€å¹¿å‘Šç­‰ï¼‰
                        skip_classes = ['ImageCaption', 'ImageCredit', 'ad-slug', 'showcase']
                        if any(skip in p_class for skip in skip_classes):
                            continue
                        
                        text = p.inner_text().strip()
                        
                        # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½å’Œå¹¿å‘Šç›¸å…³æ–‡æœ¬
                        if text and len(text) > 20:
                            # æ’é™¤å¸¸è§çš„éæ­£æ–‡å†…å®¹
                            skip_texts = ['advertisement', 'follow', 'twitter', '@', 'getty images', 'photo:']
                            if not any(skip.lower() in text.lower()[:50] for skip in skip_texts):
                                paragraphs.append(text)
                    except:
                        continue
        
        article_data["content"] = "\n\n".join(paragraphs)
        article_data["paragraph_count"] = len(paragraphs)
        
        return article_data
        
    except Exception as e:
        return {
            "url": url,
            "error": str(e),
            "scraped_at": datetime.now().isoformat(),
        }


def save_article(article: dict, output_dir: Path) -> Path:
    """
    ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶
    
    Args:
        article: æ–‡ç« æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶åï¼šä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    title = article.get("title", "untitled")
    safe_title = "".join(c if c.isalnum() or c == "_" else "_" if c == " " else "" for c in title)
    # åˆå¹¶è¿ç»­çš„ä¸‹åˆ’çº¿ï¼Œå¹¶é™åˆ¶é•¿åº¦
    safe_title = "_".join(part for part in safe_title.split("_") if part)[:80]
    filename = f"{safe_title}.json"
    
    filepath = output_dir / filename
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(article, f, ensure_ascii=False, indent=2)
    
    return filepath


def is_ci_environment() -> bool:
    """æ£€æµ‹æ˜¯å¦åœ¨ CI ç¯å¢ƒä¸­è¿è¡Œ"""
    return os.environ.get("CI") == "true" or os.environ.get("GITHUB_ACTIONS") == "true"


def launch_browser(p, with_cookie: bool = False):
    """
    å¯åŠ¨æµè§ˆå™¨ï¼Œå¯é€‰æ‹©æ€§åœ°åŠ è½½å·²ä¿å­˜çš„ Cookie
    åœ¨ CI ç¯å¢ƒä¸­è‡ªåŠ¨ä½¿ç”¨ headless æ¨¡å¼
    """
    browser = None
    is_ci = is_ci_environment()
    headless = is_ci  # CI ç¯å¢ƒä½¿ç”¨ headless æ¨¡å¼
    
    if is_ci:
        print("ğŸ¤– æ£€æµ‹åˆ° CI ç¯å¢ƒï¼Œä½¿ç”¨ headless æ¨¡å¼")
    
    # åœ¨ CI ç¯å¢ƒä¸­ä¼˜å…ˆä½¿ç”¨ Chromiumï¼ˆå·²å®‰è£…ï¼‰
    browser_order = [(p.chromium, "Chromium")] if is_ci else [
        (p.firefox, "Firefox"), 
        (p.webkit, "WebKit"), 
        (p.chromium, "Chromium")
    ]
    
    # å°è¯•å¯åŠ¨æµè§ˆå™¨
    for browser_type, name in browser_order:
        try:
            print(f"å°è¯•å¯åŠ¨ {name} æµè§ˆå™¨...", flush=True)
            if name == "Chromium":
                # CI ç¯å¢ƒéœ€è¦æ›´å¤šå‚æ•°æ¥ç¡®ä¿ç¨³å®šæ€§
                chromium_args = [
                    '--disable-gpu',
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-setuid-sandbox',
                    '--disable-background-networking',
                    '--disable-default-apps',
                    '--disable-extensions',
                    '--disable-sync',
                    '--disable-translate',
                    '--metrics-recording-only',
                    '--no-first-run',
                    '--safebrowsing-disable-auto-update',
                ]
                browser = browser_type.launch(
                    headless=headless,
                    slow_mo=0 if is_ci else 100,
                    args=chromium_args
                )
            else:
                browser = browser_type.launch(headless=headless, slow_mo=0 if is_ci else 100)
            print(f"âœ“ {name} å¯åŠ¨æˆåŠŸ", flush=True)
            break
        except Exception as e:
            print(f"{name} å¯åŠ¨å¤±è´¥: {e}", flush=True)
    
    if browser is None:
        print("æ‰€æœ‰æµè§ˆå™¨éƒ½æ— æ³•å¯åŠ¨ï¼Œè¯·è¿è¡Œ: playwright install chromium")
        return None, None
    
    # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œå¯é€‰æ‹©æ€§åœ°åŠ è½½ Cookie
    context_options = {
        "viewport": {"width": 1920, "height": 1080},
        "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    if with_cookie and COOKIE_FILE.exists():
        context_options["storage_state"] = str(COOKIE_FILE)
        print(f"âœ“ å·²åŠ è½½ Cookie: {COOKIE_FILE}")
    
    context = browser.new_context(**context_options)
    return browser, context


def main():
    """
    ä¸»å‡½æ•°
    
    ä½¿ç”¨æ–¹æ³•:
      python scraper.py --login   # æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie
      python scraper.py           # ä½¿ç”¨å·²ä¿å­˜çš„ Cookie è¿›è¡Œçˆ¬å–
    """
    print("[DEBUG] main() å‡½æ•°å¼€å§‹æ‰§è¡Œ", flush=True)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    login_mode = "--login" in sys.argv
    debug_mode = "--debug" in sys.argv  # è°ƒè¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€ç¯‡æ–‡ç« 
    save_html = "--save-html" in sys.argv  # ä¿å­˜HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•
    
    is_ci = is_ci_environment()
    print(f"[DEBUG] CI ç¯å¢ƒ: {is_ci}", flush=True)
    print(f"[DEBUG] Cookie æ–‡ä»¶å­˜åœ¨: {COOKIE_FILE.exists()}", flush=True)
    
    print("=" * 60)
    print("The Athletic æ–‡ç« çˆ¬è™«")
    print("=" * 60)
    
    print("[DEBUG] å‡†å¤‡å¯åŠ¨ Playwright...", flush=True)
    with sync_playwright() as p:
        print("[DEBUG] Playwright å¯åŠ¨æˆåŠŸ", flush=True)
        
        # ç™»å½•æ¨¡å¼ï¼šæ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie
        if login_mode:
            manual_login_and_save_cookie(p)
            return
        
        # çˆ¬å–æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ Cookie
        if not has_saved_cookie():
            print("âŒ æœªæ‰¾åˆ°å·²ä¿å­˜çš„ Cookie")
            print("")
            print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œç™»å½•ï¼š")
            print("  python scraper.py --login")
            print("")
            return
        
        # å¯åŠ¨æµè§ˆå™¨å¹¶åŠ è½½ Cookie
        browser, context = launch_browser(p, with_cookie=True)
        if browser is None:
            return
        
        page = context.new_page()
        
        try:
            # éªŒè¯ç™»å½•çŠ¶æ€
            print("ğŸ” éªŒè¯ç™»å½•çŠ¶æ€...", flush=True)
            page.goto("https://www.nytimes.com/athletic/", wait_until="domcontentloaded", timeout=60000)
            time.sleep(2)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•çŠ¶æ€ï¼‰æˆ–ç”¨æˆ·èœå•ï¼ˆå·²ç™»å½•çŠ¶æ€ï¼‰
            login_button = page.locator('a[href*="/login"], button:has-text("Log In"), a:has-text("Log In")')
            subscribe_button = page.locator('a[href*="/subscribe"], button:has-text("Subscribe")')
            
            # æ£€æŸ¥é¡µé¢ä¸Šçš„ç™»å½•/è®¢é˜…æŒ‰é’®
            has_login = login_button.count() > 0
            has_subscribe = subscribe_button.count() > 0
            
            # æ‰“å°å½“å‰é¡µé¢çš„ cookies
            cookies = context.cookies()
            athletic_cookies = [c for c in cookies if 'athletic' in c.get('domain', '') or 'nytimes' in c.get('domain', '')]
            print(f"  å½“å‰ä¼šè¯ cookies æ•°é‡: {len(cookies)} (athletic/nytimesç›¸å…³: {len(athletic_cookies)})", flush=True)
            
            if has_login or has_subscribe:
                print("âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°ç™»å½•/è®¢é˜…æŒ‰é’®ï¼Œå¯èƒ½æœªæˆåŠŸç™»å½•ï¼", flush=True)
                print("  è¯·æ£€æŸ¥ AUTH_STATE_JSON secret æ˜¯å¦æ­£ç¡®è®¾ç½®", flush=True)
            else:
                print("âœ“ ç™»å½•çŠ¶æ€éªŒè¯é€šè¿‡", flush=True)
            
            # åŠ è½½æ–‡ç« ç´¢å¼•ï¼ˆç”¨äºå»é‡ï¼‰
            index = load_index()
            print(f"âœ“ å·²åŠ è½½ç´¢å¼•ï¼Œå†å²æŠ“å–æ–‡ç« æ•°: {len(index)}")
            
            # è·å–æ–‡ç« é“¾æ¥
            articles = get_article_links(page, debug=save_html)
            
            if not articles:
                print("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
                return
            
            # è¿‡æ»¤å·²æŠ“å–çš„æ–‡ç« 
            new_articles = []
            skipped_count = 0
            for article in articles:
                if is_article_scraped(index, article["url"]):
                    skipped_count += 1
                else:
                    new_articles.append(article)
            
            print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {skipped_count} ç¯‡å·²æŠ“å–ï¼Œ{len(new_articles)} ç¯‡å¾…æŠ“å–")
            
            if not new_articles:
                print("æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æŠ“å–")
                return
            
            # æå–æ¯ç¯‡æ–‡ç« å†…å®¹
            print("\nå¼€å§‹æå–æ–‡ç« å†…å®¹...")
            print("-" * 40)
            
            # è°ƒè¯•æ¨¡å¼åªæŠ“å–ç¬¬ä¸€ç¯‡
            if debug_mode:
                print("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€ç¯‡æ–‡ç« ")
                new_articles = new_articles[:1]
            
            all_articles = []
            success_count = 0
            
            for i, article_info in enumerate(new_articles, 1):
                print(f"[{i}/{len(new_articles)}] æ­£åœ¨æå–: {article_info['title'][:50]}...")
                
                # åªåœ¨æŒ‡å®š --save-html å‚æ•°ä¸”æ˜¯ç¬¬ä¸€ç¯‡æ–‡ç« æ—¶ä¿å­˜HTML
                should_save_html = save_html and (i == 1)
                article_data = extract_article_content(page, article_info["url"], save_html=should_save_html)
                all_articles.append(article_data)
                
                content_len = len(article_data.get("content", ""))
                if "error" in article_data:
                    print(f"  âœ— æå–å¤±è´¥: {article_data['error']}")
                else:
                    # æ ¹æ®å‘å¸ƒæ—¥æœŸç¡®å®šå­˜å‚¨ç›®å½•
                    published_date = article_data.get("published_date", "")
                    output_dir = get_output_dir_by_date(published_date)
                    
                    # ä¿å­˜å•ç¯‡æ–‡ç« 
                    filepath = save_article(article_data, output_dir)
                    
                    print(f"  âœ“ å·²ä¿å­˜: {output_dir.name}/{filepath.name} ({content_len} å­—ç¬¦)")
                    success_count += 1
                    
                    # æ›´æ–°ç´¢å¼•
                    index[article_info["url"]] = datetime.now().isoformat()
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)
            
            # ä¿å­˜ç´¢å¼•
            save_index(index)
            print(f"âœ“ ç´¢å¼•å·²æ›´æ–°ï¼Œå½“å‰æ€»æ–‡ç« æ•°: {len(index)}")
            
            print("\n" + "=" * 60)
            print(f"âœ“ çˆ¬å–å®Œæˆ!")
            print(f"  - æœ¬æ¬¡æŠ“å–: {len(new_articles)} ç¯‡")
            print(f"  - æˆåŠŸæå–: {success_count} ç¯‡")
            print(f"  - è·³è¿‡å·²æŠ“å–: {skipped_count} ç¯‡")
            print(f"  - è¾“å‡ºç›®å½•: {ARTICLES_DIR.absolute()} (æŒ‰å‘å¸ƒæ—¥æœŸåˆ†ç›®å½•)")
            print(f"  - ç´¢å¼•æ–‡ä»¶: {INDEX_FILE.absolute()}")
            print("=" * 60)
            
        finally:
            browser.close()


if __name__ == "__main__":
    main()

